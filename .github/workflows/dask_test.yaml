name: Tests / dask/dask

on:
  push:
    branches:
      - main
  pull_request:
  workflow_dispatch:

# When this workflow is queued, automatically cancel any previous running
# or pending jobs from the same branch
concurrency:
  group: dask_test-${{ github.ref }}
  cancel-in-progress: true

# Required shell entrypoint to have properly activated conda environments
defaults:
  run:
    shell: bash -l {0}

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.9", "3.12"]
        environment-file: [ci/environment.yml]

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Needed by codecov.io

      - name: Get current date
        id: date
        run: echo "date=$(date +%Y-%m-%d)" >> "${GITHUB_OUTPUT}"

      - name: Install Environment
        uses: mamba-org/setup-micromamba@v1
        with:
          environment-file: ${{ matrix.environment-file }}
          create-args: python=${{ matrix.python-version }}
          # Wipe cache every 24 hours or whenever environment.yml changes. This means it
          # may take up to a day before changes to unpinned packages are picked up.
          # To force a cache refresh, change the hardcoded numerical suffix below.
          cache-environment-key: environment-${{ steps.date.outputs.date }}-0

      - name: Install current main versions of dask
        run: python -m pip install git+https://github.com/dask/dask

      - name: Install current main versions of distributed
        run: python -m pip install git+https://github.com/dask/distributed

      - name: Install dask-expr
        run: python -m pip install -e . --no-deps

      - name: Print dask versions
        # Output of `micromamba list` is buggy for pip-installed packages
        run: pip list | grep -E 'dask|distributed'

      - name: Run Dask DataFrame tests
        run: python -c "import dask.dataframe as dd; dd.test_dataframe()"
